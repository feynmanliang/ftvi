{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crude-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "params = {\n",
    "    'axes.labelsize': 8,\n",
    "    'font.size': 8,\n",
    "    'legend.fontsize': 10,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [4.5, 4.5],\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': True,\n",
    "}\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "mpl.rcParams.update(params)\n",
    "mpl.use(\"pgf\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-gravity",
   "metadata": {},
   "source": [
    "# Define a fat 2D pancake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-webcam",
   "metadata": {},
   "source": [
    "Consider a fat-tailed 2D pancake defined by the product distribution\n",
    "$$StudentT(\\nu=1) \\otimes N(0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outside-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "cmap = 'YlGnBu'\n",
    "\n",
    "class Pancake(dist.Distribution):\n",
    "    support = dist.constraints.real\n",
    "    \n",
    "    def __init__(self, fat_dims=1, thin_dims=1, df=torch.ones(1)):\n",
    "        self.fat_dim_dist = dist.StudentT(df=torch.ones(fat_dims))\n",
    "        self.thin_dim_dist = dist.Normal(loc=torch.zeros(thin_dims), scale=torch.ones(thin_dims))\n",
    "        self.fat_dims = fat_dims\n",
    "        self.thin_dims = thin_dims\n",
    "        self._event_shape = [fat_dims + thin_dims]\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        return torch.stack([\n",
    "            self.fat_dim_dist.sample(sample_shape),\n",
    "            self.thin_dim_dist.sample(sample_shape),\n",
    "        ], dim=1).squeeze()\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        fat, thin = x.split(self.fat_dims, dim=-1)\n",
    "        return self.fat_dim_dist.log_prob(fat) + self.thin_dim_dist.log_prob(thin)\n",
    "target = Pancake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blind-sullivan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def estimate_density(d, num_samples=1000):\n",
    "    data = target.sample((num_samples,))\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    xmin, xmax = -10, 10\n",
    "    ymin, ymax = -5, 5\n",
    "\n",
    "    # Peform the kernel density estimate\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = scipy.stats.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions), xx.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    im = ax.imshow(np.rot90(f), cmap=cmap, extent=[xmin, xmax, ymin, ymax])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_xlabel('StudentT(df=1)')\n",
    "    ax.set_ylabel('Normal(0,1)')\n",
    "    return fig\n",
    "\n",
    "def plot_density(d, ax=None, bounds=[-10, 10, -5, 5], norm=None, exp=False):\n",
    "    #ounds = [-6, 4, 0, 5]\n",
    "    ax.grid(False)\n",
    "    xmin, xmax, ymin, ymax = bounds\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(xmin, xmax, 100),\n",
    "        torch.linspace(ymin, ymax, 100),\n",
    "    )\n",
    "    f = d.log_prob(torch.stack((xx, yy), dim=-1).reshape(-1, 2)).reshape((100,100)).detach().numpy()\n",
    "    if exp:\n",
    "        f = np.exp(f)\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca()\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    if norm:\n",
    "        im = ax.imshow(np.rot90(f), cmap=cmap, norm=norm, extent=[xmin, xmax, ymin, ymax], aspect=\"auto\")\n",
    "    else:\n",
    "        im = ax.imshow(np.rot90(f), cmap=cmap, extent=[xmin, xmax, ymin, ymax], aspect=\"auto\")\n",
    "    #fig.colorbar(im, ax=ax)\n",
    "    ax.contour(xx, yy, f, colors='w', linestyles='dashed')\n",
    "    #ax.set_ylabel('$\\\\sigma$')\n",
    "    #ax.set_xlabel('$\\\\mu$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extraordinary-parade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#estimate_density(target, num_samples=int(1e3)).show()\n",
    "#fig = plot_density(target, norm=plt.Normalize(-5, -2), bounds=[-6, 4, -5, 5])\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(5.5,3))\n",
    "plot_density(target,\n",
    "             bounds=[-10, 10, -5, 5],\n",
    "             ax=axs[0][0],\n",
    "                   #norm=plt.Normalize(0, 0.1),\n",
    "                   #bounds=[-6, 4, -5, 5], exp=True)\n",
    "                  )\n",
    "#fig = plot_density(target, bounds=[-10, 10, -5, 5])\n",
    "axs[0][0].set_title('Target')\n",
    "axs[0][0].set_xlabel('StudentT($\\\\nu=1$)')\n",
    "axs[0][0].set_ylabel('Normal($\\\\mu=0,\\\\sigma^2=1$)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-stephen",
   "metadata": {},
   "source": [
    "## Approximate it using ADVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-sandwich",
   "metadata": {},
   "source": [
    "First consider the variational family $q_{\\mu \\in \\mathbb{R}^2,\\sigma \\in \\mathbb{R}^2_{>0}} = \\mu + \\sigma \\odot N(0, I_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerical-auction",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e205acdc824c278c8fe6eb0a3268df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training iterations'), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor([0.2093], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import beanmachine.ppl as bm\n",
    "import flowtorch.bijectors\n",
    "import flowtorch.params\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@bm.random_variable\n",
    "def pancake():\n",
    "    return Pancake()\n",
    "\n",
    "from beanmachine.ppl.experimental.vi.variational_infer import (\n",
    "    MeanFieldVariationalInference,\n",
    ")\n",
    "\n",
    "def on_iter(it, loss, vi_dicts):\n",
    "    if it % 10 == 0:\n",
    "        tqdm.write(f\"Loss: {loss}\", end='')\n",
    "\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=1000,\n",
    "    progress_bar=True,\n",
    "    flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "        flowtorch.params.DenseAutoregressive(hidden_dims=(8,8))\n",
    "    ),\n",
    "    lr=1e-2,\n",
    "    on_iter=on_iter,\n",
    ")\n",
    "vapprox_advi = vi_dicts(pancake())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forced-studio",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ADVI')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estimate_density(vapprox_advi).show()\n",
    "plot_density(vapprox_advi,\n",
    "                   ax=axs[0][1],\n",
    "                   bounds=[-10, 10, -5, 5],\n",
    "                   #norm=plt.Normalize(0, 0.1), bounds=[-6, 4, -5, 5], exp=True\n",
    "                  )\n",
    "                   \n",
    "#fig = plot_density(vapprox_advi, bounds=[-6, 4, -5, 5])\n",
    "axs[0][1].set_title('ADVI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-budapest",
   "metadata": {},
   "source": [
    "The resulting Normal approximation has mean $\\approx (0,0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean: Parameter containing:\n",
      "tensor([ 0.0544, -0.0957], requires_grad=True)\n",
      "Var: tensor([3.0773, 4.8643], grad_fn=<PowBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Mean: {vapprox_advi.new_dist.base_dist.base_dist.mean}\n",
    "Var: {vapprox_advi.new_dist.base_dist.base_dist.variance}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-encoding",
   "metadata": {},
   "source": [
    "Its variance is higher in the heavy-tailed direction, but it does not capture the tail decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-midwest",
   "metadata": {},
   "source": [
    "## Approximate with ATAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-contribution",
   "metadata": {},
   "source": [
    "Let us now use FTVI, which uses $q_{\\nu \\in R_+^2, \\mu \\in \\mathbb{R}^2, \\sigma \\in R^2_+} = \\mu + \\sigma \\odot StudentT(\\nu)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chief-jumping",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89e2ac61b8e43c1b47a91308af75bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training iterations'), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor([0.0003], grad_fn=<SubBackward0>), DF: tensor([ 1.0060, 12.3989], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import beanmachine.ppl as bm\n",
    "import flowtorch.bijectors\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from beanmachine.ppl.experimental.vi.variational_infer import (\n",
    "    MeanFieldVariationalInference,\n",
    ")\n",
    "\n",
    "def on_iter(it, loss, vi_dicts):\n",
    "    if it % 10 == 0:\n",
    "        tqdm.write(f\"Loss: {loss}, DF: {vi_dicts(pancake()).new_dist.base_dist.base_dist.df}\", end='')\n",
    "\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=1000,\n",
    "    progress_bar=True,\n",
    "    flow=lambda: flowtorch.bijectors.Compose([]),\n",
    "#     flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "#         flowtorch.params.DenseAutoregressive(hidden_dims=(8,8))\n",
    "#     ),\n",
    "    lr=1e-2,\n",
    "    on_iter=on_iter,\n",
    "    base_dist=dist.StudentT,\n",
    "    base_args={\n",
    "        'df': nn.Parameter(torch.tensor([5.0]).log()),\n",
    "        'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "        'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "    }\n",
    ")\n",
    "vapprox_ataf = vi_dicts(pancake())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "given-pressure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ATAF')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_density(vapprox_ataf,\n",
    "                   ax=axs[1][1],\n",
    "                   bounds=[-10, 10, -5, 5],\n",
    "                   #norm=plt.Normalize(0, 0.1), bounds=[-6, 4, -5, 5], exp=True\n",
    "                  )\n",
    "axs[1][1].set_title('ATAF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-ebony",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Approximate with TAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-resort",
   "metadata": {},
   "source": [
    "The previous SOTA for heavy-tailed density estimation uses a single tail-index for a high-dimensional distribution, that is $\\nu > 0$ rather than $\\nu \\in R^2_+$.\n",
    "Everything else is identical to FTVI.\n",
    "\n",
    "This requires a bit of hacking to get working; here we patch `vapprox` to have a scalar `df` and hard-code $\\mu = 0$ and $\\sigma = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compressed-underground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5.], grad_fn=<ExpandBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# initialize a MFVApprox\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=1,\n",
    "    progress_bar=False,\n",
    "#     flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "#         flowtorch.params.DenseAutoregressive(hidden_dims=(8,8))\n",
    "#     ),\n",
    "    flow=lambda: flowtorch.bijectors.Compose([]),\n",
    "    lr=1e-2,\n",
    "    base_dist=dist.StudentT,\n",
    "    base_args={\n",
    "        'df': nn.Parameter(torch.tensor([5.0]).log()),\n",
    "        'loc': torch.zeros(2),\n",
    "        'scale': torch.ones(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# monkey patch it to do TAF\n",
    "vapprox_taf_fixed = vi_dicts(pancake())\n",
    "vapprox_taf_fixed.base_dist = dist.StudentT\n",
    "vapprox_taf_fixed.base_args = {\n",
    "    'df': nn.Parameter(vapprox_taf_fixed.base_args['df'].mean()),\n",
    "    'loc': torch.zeros(2),\n",
    "    'scale': torch.ones(2).log(),\n",
    "}\n",
    "print(vapprox_taf_fixed.recompute_transformed_distribution().df)\n",
    "#print(\"Params: \", list(vapprox_taf_fixed.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-transcription",
   "metadata": {},
   "source": [
    "Next, we manually write the ELBO training loop (beanmachine doesn't propagate gradients for some reason):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "drawn-motion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855d9f6f6f474e37a7fa14603a95a617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27453795075416565, DF: tensor([6.0400, 6.0400], grad_fn=<ExpandBackward>)>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "\n",
    "#optimizer = torch.optim.Adam([vapprox_taf_fixed.base_args['df']], lr=1e-2)\n",
    "optimizer = torch.optim.Adam(vapprox_taf_fixed.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    M = 100\n",
    "    zk = vapprox_taf_fixed.rsample((M,))\n",
    "    elbo = Pancake().log_prob(zk).sum()\n",
    "    #elbo = BLR().log_prob(zk).sum()\n",
    "    elbo -= vapprox_taf_fixed.log_prob(zk).sum()\n",
    "    elbo /= M\n",
    "    \n",
    "    loss = -elbo\n",
    "    loss.backward(retain_graph=True)\n",
    "    tqdm.write(f\"Loss: {loss.item()}, DF: {vapprox_taf_fixed.new_dist.base_dist.df}\", end='')\n",
    "    #print(vapprox_taf_fixed.base_args['df'])\n",
    "    optimizer.step()\n",
    "    vapprox_taf_fixed.recompute_transformed_distribution()\n",
    "    #print(vapprox_taf_fixed.base_args['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outer-produce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'TAF')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = axs[1][0]\n",
    "#fig, ax = plt.subplots()\n",
    "plot_density(vapprox_taf_fixed,\n",
    "                   ax=ax,\n",
    "                   bounds=[-10, 10, -5, 5],\n",
    "                   #norm=plt.Normalize(0, 0.1), bounds=[-6, 4, -5, 5], exp=True\n",
    "                  )\n",
    "ax.set_title('TAF')\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "understanding-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('pancake.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-sweet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
