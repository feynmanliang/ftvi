{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "subtle-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alternate-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "params = {\n",
    "    'axes.labelsize': 8,\n",
    "    'font.size': 8,\n",
    "    'legend.fontsize': 10,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'text.usetex': False,\n",
    "    'figure.figsize': [4.5, 4.5],\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': True,\n",
    "}\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "mpl.rcParams.update(params)\n",
    "mpl.use(\"pgf\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acquired-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "cmap = 'YlGnBu'\n",
    "\n",
    "class BLR(dist.Distribution):\n",
    "    # https://en.wikipedia.org/wiki/Bayesian_linear_regression\n",
    "    support = dist.constraints.real_vector\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._event_shape = [2]\n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size([])):\n",
    "        inv_scale = dist.Gamma(1, 1/2).sample(sample_shape)\n",
    "        scale = inv_scale**-1\n",
    "        loc = dist.Normal(0, scale.sqrt()).sample()\n",
    "        scale = (2*torch.bernoulli(0.5*torch.ones(sample_shape))-1)*scale\n",
    "        return torch.stack([scale, loc], axis=0).T\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        if x.ndim == 1:\n",
    "            loc = x[1]\n",
    "            #scale = x[1].exp()\n",
    "            #scale = 1 + nn.ELU()(x[1])\n",
    "            scale = x[0].abs()\n",
    "        else:\n",
    "            loc = x[:,1]\n",
    "            #scale = x[:,1].exp()\n",
    "            #scale = 1 + nn.ELU()(x[:,1])\n",
    "            scale = x[:,0].abs()\n",
    "        return dist.Normal(0, scale.sqrt()).log_prob(loc) + dist.Gamma(1, 1/2).log_prob(scale**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "twelve-supply",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def plot_density(d, ax=None, norm=None, bounds=[-2, 5, -2, 2], exp=True):\n",
    "    ax.grid(False)\n",
    "    xmin, xmax, ymin, ymax = bounds\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(xmin, xmax, 100),\n",
    "        torch.linspace(ymin, ymax, 100),\n",
    "    )\n",
    "    f = d.log_prob(torch.stack((xx, yy), dim=-1).reshape(-1, 2)).reshape((100,100)).detach().numpy()\n",
    "    if exp:\n",
    "        f = np.exp(f)\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca()\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    im = ax.imshow(np.rot90(f), cmap=cmap, norm=norm, extent=[xmin, xmax, ymin, ymax], aspect='auto')\n",
    "    #ax.contour(xx, yy, f, norm=norm, colors='w', linestyles='dashed')\n",
    "    ax.set_xlabel('$\\\\sigma$')\n",
    "    ax.set_ylabel('$\\\\beta$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-fellow",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# BLR for gaussian (loc) + fat tail (scale) product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-theme",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'grid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea2a2c5ac9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m ax = plot_density(target,\n\u001b[0m\u001b[1;32m     11\u001b[0m                   )\n\u001b[1;32m     12\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ground Truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4fe785212968>\u001b[0m in \u001b[0;36mplot_density\u001b[0;34m(d, ax, norm, bounds, exp)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     xx, yy = torch.meshgrid(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'grid'"
     ]
    }
   ],
   "source": [
    "# mu = 2*torch.randn(2)\n",
    "# target = dist.MixtureSameFamily(\n",
    "#     dist.Categorical(torch.tensor([0.5, 0.5])),\n",
    "#     dist.MultivariateNormal(\n",
    "#         torch.stack([mu, -mu]), \n",
    "#         torch.stack([torch.eye(2), torch.eye(2)])),\n",
    "# )\n",
    "target = BLR()\n",
    "\n",
    "ax = plot_density(target,\n",
    "                  )\n",
    "ax.set_title('Ground Truth')\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tails along x and y axis\n",
    "xs = torch.linspace(int(1e4), int(1e6), steps=100)\n",
    "x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(xs, target.log_prob(x_axis).detach().numpy())\n",
    "ax[0].set_title('$\\\\langle e_0, X-[0;1] \\\\rangle $')\n",
    "#ax[0].set_yscale('symlog')\n",
    "\n",
    "y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "ax[1].plot(xs, target.log_prob(y_axis).detach().numpy())\n",
    "ax[1].set_title('$\\\\langle e_1, X-[0;1] \\\\rangle $')\n",
    "#ax[1].set_yscale('symlog')\n",
    "fig.suptitle('Tail behavior for BLR posterior')\n",
    "\n",
    "#fig.legend(['vapprox', 'Target'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = BLR().sample((1000,)).numpy()\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].scatter(X[:,0], X[:,1])\n",
    "ax[1].scatter(X[:,0], X[:,1])\n",
    "ax[1].set_xlim([-6, 4])\n",
    "ax[1].set_ylim([-5, 5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-riding",
   "metadata": {},
   "source": [
    "## Approximate it using ADVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-netherlands",
   "metadata": {},
   "source": [
    "First consider the variational family $q_{\\mu \\in \\mathbb{R}^2,\\sigma \\in \\mathbb{R}^2_{>0}} = \\mu + \\sigma \\odot N(0, I_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-operation",
   "metadata": {},
   "source": [
    "## Nflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-bankruptcy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from nflows import transforms, distributions, flows\n",
    "\n",
    "# Define an invertible transformation.\n",
    "transform = transforms.CompositeTransform([\n",
    "    transforms.MaskedAffineAutoregressiveTransform(features=2, hidden_features=256),\n",
    "    transforms.RandomPermutation(features=2),\n",
    "    transforms.MaskedAffineAutoregressiveTransform(features=2, hidden_features=256),\n",
    "    transforms.RandomPermutation(features=2),\n",
    "    transforms.MaskedAffineAutoregressiveTransform(features=2, hidden_features=256),\n",
    "    #transforms.MaskedAffineAutoregressiveTransform(features=2, hidden_features=256),\n",
    "])\n",
    "\n",
    "# Define a base distribution.\n",
    "base_distribution = distributions.StandardNormal(shape=[2])\n",
    "\n",
    "\n",
    "# Combine into a flow.\n",
    "flow = flows.Flow(transform=transform, distribution=base_distribution)\n",
    "\n",
    "optimizer = optim.Adam(transform.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    samples = flow.sample(1000)\n",
    "    #samples = target.sample((1000,))\n",
    "    log_q = flow.log_prob(samples)\n",
    "    log_p = target.log_prob(samples)\n",
    "    loss = (log_q - log_p).mean()\n",
    "    #loss = -log_q.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tqdm.write(f\"Loss: {loss}\", end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-philosophy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plot_density(flow,\n",
    "                   #norm=plt.Normalize(-0.00, 0.0001),\n",
    "                   bounds=[-6, 4, -5, 5], \n",
    "                   norm=plt.Normalize(-0., 0.005)\n",
    "                   #bounds=[-30, 30, -200, 200]\n",
    "                  )\n",
    "#fig = plot_density(vapprox_advi, bounds=[-6, 4, -5, 5])\n",
    "fig.axes[0].set_title('ADVI')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-puppy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at tails along x and y axis\n",
    "xs = torch.linspace(int(1e4), int(1e6), steps=100)\n",
    "x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(xs, flow.log_prob(x_axis).detach().numpy())\n",
    "ax[0].plot(xs, target.log_prob(x_axis).detach().numpy())\n",
    "ax[0].set_title('$\\\\langle e_0, X-[0;1] \\\\rangle $')\n",
    "#ax[0].set_yscale('symlog')\n",
    "\n",
    "\n",
    "y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "ax[1].plot(xs, flow.log_prob(y_axis).detach().numpy())\n",
    "ax[1].plot(xs, target.log_prob(y_axis).detach().numpy())\n",
    "ax[1].set_title('$\\\\langle e_1, X-[0;1] \\\\rangle $')\n",
    "#ax[1].set_yscale('symlog')\n",
    "\n",
    "\n",
    "fig.suptitle('Tail behavior for BLR posterior')\n",
    "fig.legend(['Flow', 'Target'])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-lucas",
   "metadata": {},
   "source": [
    "## Use beanmachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-moisture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import beanmachine.ppl as bm\n",
    "import flowtorch.bijectors\n",
    "import flowtorch.params\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@bm.random_variable\n",
    "def pancake():\n",
    "    return BLR()\n",
    "\n",
    "from beanmachine.ppl.experimental.vi.variational_infer import (\n",
    "    MeanFieldVariationalInference,\n",
    ")\n",
    "\n",
    "def on_iter(it, loss, vi_dicts):\n",
    "    if it % 10 == 0:\n",
    "        tqdm.write(f\"Loss: {loss}\", end='')\n",
    "\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=int(1),\n",
    "    progress_bar=True,\n",
    "    flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "        flowtorch.params.DenseAutoregressive(hidden_dims=(32,32,32))\n",
    "    ),\n",
    "#     base_dist=dist.Normal,\n",
    "#     base_args={\n",
    "#         'loc': 0.0,\n",
    "#         'scale': 5.0,\n",
    "#     },\n",
    "    lr=1e-2,\n",
    "    on_iter=on_iter,\n",
    "    num_elbo_mc_samples=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "target = BLR()\n",
    "tdist = vi_dicts(pancake()).new_dist\n",
    "flow_params = vi_dicts(pancake())._flow_params\n",
    "\n",
    "optimizer = optim.Adam(flow_params.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in tqdm(range(int(1e3))):\n",
    "    optimizer.zero_grad()\n",
    "    samples = tdist.rsample((10000,))\n",
    "    #samples = target.sample((1000,))\n",
    "    log_q = tdist.log_prob(samples)\n",
    "    log_p = target.log_prob(samples)\n",
    "    loss = (log_q - log_p).mean()\n",
    "    #loss = -log_q.mean()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    tqdm.write(f\"Loss: {loss}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-brown",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "vapprox_advi = copy.copy(vi_dicts(pancake()))\n",
    "#estimate_density(vapprox_advi).show()\n",
    "ax = plot_density(vapprox_advi,\n",
    "                   #norm=plt.Normalize(-0.01, 0.05),\n",
    "                  )\n",
    "#fig = plot_density(vapprox_advi, bounds=[-6, 4, -5, 5])\n",
    "ax.set_title('ADVI')\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tails along x and y axis\n",
    "xs = torch.linspace(int(1e4), int(1e5), steps=100)\n",
    "x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(xs, vapprox_advi.log_prob(x_axis).detach().numpy())\n",
    "#ax[0].plot(xs, target.log_prob(x_axis).detach().numpy())\n",
    "ax[0].set_title('$\\\\langle e_0, X-[0;1] \\\\rangle $')\n",
    "#ax[0].set_yscale('symlog')\n",
    "\n",
    "y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "ax[1].plot(xs, vapprox_advi.log_prob(y_axis).detach().numpy())\n",
    "#ax[1].plot(xs, target.log_prob(y_axis).detach().numpy())\n",
    "ax[1].set_title('$\\\\langle e_1, X-[0;1] \\\\rangle $')\n",
    "#ax[1].set_yscale('symlog')\n",
    "fig.suptitle('Tail behavior for BLR posterior')\n",
    "\n",
    "fig.legend(['vapprox', 'Target'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Mean: {vapprox_advi.new_dist.base_dist.base_dist.mean}\n",
    "Var: {vapprox_advi.new_dist.base_dist.base_dist.variance}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-interpretation",
   "metadata": {},
   "source": [
    "Its variance is higher in the heavy-tailed direction, but it does not capture the tail decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-rescue",
   "metadata": {},
   "source": [
    "## Approximate with ATAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-injection",
   "metadata": {},
   "source": [
    "Let us now use ATAF, which uses $q_{\\nu \\in R_+^2, \\mu \\in \\mathbb{R}^2, \\sigma \\in R^2_+} = \\mu + \\sigma \\odot StudentT(\\nu)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-indonesia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import beanmachine.ppl as bm\n",
    "import flowtorch.bijectors\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from beanmachine.ppl.experimental.vi.variational_infer import (\n",
    "    MeanFieldVariationalInference,\n",
    ")\n",
    "\n",
    "def on_iter(it, loss, vi_dicts):\n",
    "    if it % 10 == 0:\n",
    "        tqdm.write(f\"Loss: {loss}, DF: {vi_dicts(pancake()).new_dist.base_dist.base_dist.df}\", end='')\n",
    "\n",
    "vi_dicts(pancake()).base_dist = lambda **kwargs: dist.Independent(dist.StudentT(**kwargs), 1)\n",
    "vi_dicts(pancake()).base_arg_constraints = dist.StudentT.arg_constraints\n",
    "vi_dicts(pancake()).base_args = {\n",
    "    'df': torch.tensor([int(1e5), int(1e0)]).log(),\n",
    "    'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "    'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "}\n",
    "vi_dicts(pancake()).recompute_transformed_distribution()\n",
    "\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=int(0),\n",
    "    progress_bar=True,\n",
    "    flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "        flowtorch.params.DenseAutoregressive(\n",
    "            hidden_dims=(256,256,256),\n",
    "        ),\n",
    "    ),\n",
    "    #flow=lambda: flowtorch.bijectors.Compose([]),\n",
    "    vi_dicts=vi_dicts,\n",
    "    lr=1e-2,\n",
    "    on_iter=on_iter,\n",
    "    base_dist=dist.StudentT,\n",
    "    base_args={\n",
    "        'df': nn.Parameter(torch.tensor([5.0]).log()),\n",
    "        'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "        'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "    },\n",
    "    num_elbo_mc_samples=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-healthcare",
   "metadata": {},
   "source": [
    "Re-use the good initialization from ADVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_dicts(pancake()).base_dist = lambda **kwargs: dist.Independent(dist.StudentT(**kwargs), 1)\n",
    "vi_dicts(pancake()).base_arg_constraints = dist.StudentT.arg_constraints\n",
    "vi_dicts(pancake()).base_args = {\n",
    "    'df': torch.tensor([int(2), int(1e10)]).log(),\n",
    "    'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "    'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "}\n",
    "vi_dicts(pancake()).recompute_transformed_distribution()\n",
    "vapprox_ftvi = copy.copy(vi_dicts(pancake()))\n",
    "\n",
    "# look at tails along x and y axis\n",
    "xs = torch.linspace(int(1e2), int(1e3), steps=100)\n",
    "x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(xs, vapprox_ftvi.log_prob(x_axis).detach().numpy())\n",
    "#ax[0].plot(xs, target.log_prob(x_axis).detach().numpy())\n",
    "ax[0].set_title('$\\\\langle e_0, X-[0;1] \\\\rangle $')\n",
    "#ax[0].set_yscale('symlog')\n",
    "\n",
    "y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "ax[1].plot(xs, vapprox_ftvi.log_prob(y_axis).detach().numpy())\n",
    "#ax[1].plot(xs, target.log_prob(y_axis).detach().numpy())\n",
    "ax[1].set_title('$\\\\langle e_1, X-[0;1] \\\\rangle $')\n",
    "#ax[1].set_yscale('symlog')\n",
    "fig.suptitle('Tail behavior for BLR posterior')\n",
    "\n",
    "#fig.legend(['vapprox\\_ftvi', 'Target'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate_density(vapprox_advi).show()\n",
    "ax = plot_density(vapprox_ftvi, \n",
    "                   #norm=plt.Normalize(0, 0.2),\n",
    "                   #bounds=[-6, 4, -5, 5], \n",
    "                   #exp=False,\n",
    "                  )\n",
    "ax.set_title('ATAF')\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-municipality",
   "metadata": {},
   "source": [
    "Moreover, it recovers the fat-tailedness and tail-index for the fat-tailed dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Mean: {vapprox_ftvi.new_dist.base_dist.base_dist.mean}\n",
    "Var: {vapprox_ftvi.new_dist.base_dist.base_dist.variance}\n",
    "DoF: {vapprox_ftvi.new_dist.base_dist.base_dist.df}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-writing",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Approximate with TAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-brunei",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# initialize a MFVApprox\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=1,\n",
    "    progress_bar=False,\n",
    "    flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "        flowtorch.params.DenseAutoregressive(hidden_dims=(256,256,256))\n",
    "    ),\n",
    "    lr=1e-2,\n",
    "    base_dist=dist.StudentT,\n",
    "    base_args={\n",
    "        'df': nn.Parameter(torch.tensor([5.0]).log()),\n",
    "        'loc': torch.zeros(2),\n",
    "        'scale': torch.ones(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# monkey patch it to do TAF\n",
    "vapprox_taf_fixed = copy.copy(vi_dicts(pancake()))\n",
    "vapprox_taf_fixed.base_dist = dist.StudentT\n",
    "vapprox_taf_fixed.base_args = {\n",
    "    'df': nn.Parameter(vapprox_taf_fixed.base_args['df'].mean()),\n",
    "    'loc': torch.zeros(2),\n",
    "    'scale': torch.ones(2).log(),\n",
    "}\n",
    "print(vapprox_taf_fixed.recompute_transformed_distribution().df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-windows",
   "metadata": {},
   "source": [
    "Next, we manually write the ELBO training loop (beanmachine doesn't propagate gradients for some reason):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-religion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "optimizer = torch.optim.Adam(vapprox_taf_fixed.parameters(), lr=5e-3)\n",
    "for _ in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    M = 100\n",
    "    zk = vapprox_taf_fixed.rsample((M,))\n",
    "    #elbo = Pancake().log_prob(zk).sum()\n",
    "    elbo = BLR().log_prob(zk).sum()\n",
    "    elbo -= vapprox_taf_fixed.log_prob(zk).sum()\n",
    "    elbo /= M\n",
    "    \n",
    "    loss = -elbo\n",
    "    loss.backward(retain_graph=True)\n",
    "    tqdm.write(f\"Loss: {loss.item()}, DF: {vapprox_taf_fixed.new_dist.base_dist.df}\", end='')\n",
    "    #print(vapprox_taf_fixed.base_args['df'])\n",
    "    optimizer.step()\n",
    "    vapprox_taf_fixed.recompute_transformed_distribution()\n",
    "    #print(vapprox_taf_fixed.base_args['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapprox_taf_fixed = copy.copy(vi_dicts(pancake()))\n",
    "vapprox_taf_fixed.base_dist = lambda **kwargs: dist.Independent(dist.StudentT(**kwargs), 1)\n",
    "vapprox_taf_fixed.base_arg_constraints = dist.StudentT.arg_constraints\n",
    "vapprox_taf_fixed.base_args = {\n",
    "    'df': torch.tensor([int(5), int(5)]).log(),\n",
    "    'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "    'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "}\n",
    "vapprox_taf_fixed.recompute_transformed_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tails along x and y axis\n",
    "xs = torch.linspace(int(1e4), int(1e6), steps=100)\n",
    "x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(xs, vapprox_taf_fixed.log_prob(x_axis).detach().numpy())\n",
    "#ax[0].plot(xs, target.log_prob(x_axis).detach().numpy())\n",
    "ax[0].set_title('$\\\\langle e_0, X-[0;1] \\\\rangle $')\n",
    "#ax[0].set_yscale('symlog')\n",
    "\n",
    "y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "ax[1].plot(xs, vapprox_taf_fixed.log_prob(y_axis).detach().numpy())\n",
    "#ax[1].plot(xs, target.log_prob(y_axis).detach().numpy())\n",
    "ax[1].set_title('$\\\\langle e_1, X-[0;1] \\\\rangle $')\n",
    "#ax[1].set_yscale('symlog')\n",
    "fig.suptitle('Tail behavior for BLR posterior')\n",
    "\n",
    "#fig.legend(['vapprox\\_ftvi', 'Target'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-captain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = plot_density(vapprox_taf_fixed, \n",
    "                   #norm=plt.Normalize(0, 0.1),\n",
    "                   #bounds=[-6, 4, -5, 5], exp=True\n",
    "                 )\n",
    "ax.set_title('TAF')\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-advantage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Mean: {vapprox_taf_fixed.new_dist.base_dist.mean}\n",
    "Var: {vapprox_taf_fixed.new_dist.base_dist.variance}\n",
    "DoF: {vapprox_taf_fixed.new_dist.base_dist.base_dist.df}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-uncertainty",
   "metadata": {},
   "source": [
    "# Big Plot with Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "still-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_marginal(axs, d):\n",
    "    xs = torch.linspace(int(1e4), int(1e6), steps=100)\n",
    "    x_axis = torch.stack([xs, torch.ones_like(xs)]).T\n",
    "    y_axis = torch.stack([torch.ones_like(xs), xs]).T\n",
    "    axs[0].plot(xs, d.log_prob(x_axis).detach().numpy())\n",
    "    axs[1].plot(xs, d.log_prob(y_axis).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "collective-revelation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ee98168fbc4775bb32ec108c2eee4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training iterations'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor([8.7988], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea0ab0fd7148c9ba75b476944f17af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.24911706149578094376\n"
     ]
    }
   ],
   "source": [
    "import beanmachine.ppl as bm\n",
    "import flowtorch.bijectors\n",
    "import flowtorch.params\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "@bm.random_variable\n",
    "def pancake():\n",
    "    return BLR()\n",
    "\n",
    "from beanmachine.ppl.experimental.vi.variational_infer import (\n",
    "    MeanFieldVariationalInference,\n",
    ")\n",
    "\n",
    "def on_iter(it, loss, vi_dicts):\n",
    "    if it % 10 == 0:\n",
    "        tqdm.write(f\"Loss: {loss}\", end='')\n",
    "\n",
    "vi_dicts = MeanFieldVariationalInference().infer(\n",
    "    queries=[pancake()],\n",
    "    observations={},\n",
    "    num_iter=int(1),\n",
    "    progress_bar=True,\n",
    "    flow=lambda: flowtorch.bijectors.AffineAutoregressive(\n",
    "        flowtorch.params.DenseAutoregressive(hidden_dims=(32,32,32))\n",
    "    ),\n",
    "#     base_dist=dist.Normal,\n",
    "#     base_args={\n",
    "#         'loc': 0.0,\n",
    "#         'scale': 5.0,\n",
    "#     },\n",
    "    lr=1e-2,\n",
    "    on_iter=on_iter,\n",
    "    num_elbo_mc_samples=1000,\n",
    ")\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "target = BLR()\n",
    "tdist = vi_dicts(pancake()).new_dist\n",
    "flow_params = vi_dicts(pancake())._flow_params\n",
    "\n",
    "optimizer = optim.Adam(flow_params.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in tqdm(range(int(500))):\n",
    "    optimizer.zero_grad()\n",
    "    samples = tdist.rsample((1000,))\n",
    "    #samples = target.sample((1000,))\n",
    "    log_q = tdist.log_prob(samples)\n",
    "    log_p = target.log_prob(samples)\n",
    "    loss = (log_q - log_p).mean()\n",
    "    #loss = -log_q.mean()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    tqdm.write(f\"Loss: {loss}\", end='')\n",
    "\n",
    "import copy\n",
    "vapprox_advi = copy.copy(vi_dicts(pancake()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "architectural-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.transforms import offset_copy\n",
    "\n",
    "cols = [\n",
    "    '$p(\\\\beta, \\\\sigma)$',\n",
    "    '$p(\\\\beta=1, \\\\sigma)$',\n",
    "    '$p(\\\\beta, \\\\sigma=1)$',\n",
    "]\n",
    "rows = [\n",
    "    'Target',\n",
    "    'Gaussian',\n",
    "    'TAF',\n",
    "    'ATAF',\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(5.5, 4.5))\n",
    "#plt.setp(axes.flat, xlabel='X-label', ylabel='Y-label')\n",
    "\n",
    "pad = 5 # in points\n",
    "\n",
    "for row, d in enumerate([\n",
    "    target, vi_dicts(pancake()), vi_dicts(pancake()), vi_dicts(pancake()),\n",
    "]):\n",
    "    if row == 0:\n",
    "        plot_density(d, axes[row][0],\n",
    "                     norm=plt.Normalize(0.07, 0.13),\n",
    "                     bounds=[1,8,-2,2],\n",
    "                    )\n",
    "        plot_marginal(axes[row][1:3], d)\n",
    "    else:\n",
    "        plot_density(d, axes[row][0], \n",
    "                     bounds=[1,8,-2,2],\n",
    "                     #norm=plt.Normalize(-0.00, 0.5)\n",
    "                    )\n",
    "        \n",
    "        if row > 1:\n",
    "            if row == 2:\n",
    "                dfs = torch.tensor([int(5), int(5)]).log()\n",
    "            elif row == 3:\n",
    "                dfs = torch.tensor([int(1e12), int(1e0)]).log()\n",
    "\n",
    "            d.base_dist = lambda **kwargs: dist.Independent(dist.StudentT(**kwargs), 1)\n",
    "            d.base_arg_constraints = dist.StudentT.arg_constraints\n",
    "            d.base_args = {\n",
    "                'df': dfs,\n",
    "                'loc': nn.Parameter(torch.tensor([0.0])),\n",
    "                'scale': nn.Parameter(torch.tensor([1.0])),\n",
    "            }\n",
    "            d.recompute_transformed_distribution()\n",
    "        plot_marginal(axes[row][2:0:-1], d)\n",
    "        \n",
    "    axs =  axes[row][1:3]\n",
    "    axs[0].set_xlabel('$\\\\sigma$')\n",
    "    axs[1].set_xlabel('$\\\\beta$')\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.annotate(col, xy=(0.5, 1), xytext=(0, pad),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size='large', ha='center', va='baseline')\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.annotate(row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                size='large', ha='right', va='center', rotation=90)\n",
    "\n",
    "fig.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "# tight_layout doesn't take these labels into account. We'll need \n",
    "# to make some room. These numbers are are manually tweaked. \n",
    "# You could automatically calculate them, but it's a pain.\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "touched-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('blr_aniso.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-titanium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
